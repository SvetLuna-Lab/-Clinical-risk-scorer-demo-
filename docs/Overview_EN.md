# Clinical Risk Scorer Demo — Project Overview (EN)

## 1. Idea

This repository is a small **end-to-end ML demo** for clinical risk scoring
on tabular patient data.

Key goals:

- show a clean, reproducible ML pipeline in Python;
- keep the dataset fully **synthetic** (no real medical data);
- provide a minimal but realistic structure:
  - data generation and loading,
  - feature preprocessing,
  - model training and evaluation,
  - simple FastAPI service for `/predict`.

The project is **not** intended for real clinical use and must not be treated
as a medical device or decision-support tool. It is an engineering demo only.

---

## 2. Repository structure

```text
clinical-risk-scorer-demo/
├─ data/
│  ├─ raw/                      # optional, for raw CSVs or experiments
│  └─ processed/                # train.csv, test.csv
├─ configs/
│  └─ default.yaml              # config: data paths, features, target, model params
├─ src/
│  ├─ __init__.py
│  ├─ paths.py                  # centralised project/data/models/metrics paths
│  ├─ data/
│  │  ├─ generate_synthetic.py  # synthetic patient generation
│  │  └─ dataset.py             # load train/test according to YAML config
│  ├─ features/
│  │  └─ preprocess.py          # sklearn preprocessing pipeline
│  ├─ models/
│  │  ├─ train.py               # train model, compute metrics, save artifacts
│  │  └─ evaluate.py            # standalone evaluation, write metrics.json
│  └─ cli.py                    # CLI with commands: generate-data, train, evaluate
├─ api/
│  └─ app.py                    # FastAPI app exposing /health and /predict
├─ notebooks/
│  └─ 01_exploration.ipynb      # basic EDA of the synthetic dataset
├─ tests/
│  ├─ test_data_generation.py   # unit test for synthetic data generator
│  ├─ test_pipeline_smoke.py    # end-to-end pipeline smoke test
│  └─ test_api_predict.py       # smoke test for /predict endpoint
├─ models/                      # trained sklearn pipelines (.joblib)
├─ metrics/                     # metrics JSON files
├─ docs/
│  ├─ Overview_RU.md            # Russian technical overview
│  └─ Overview_EN.md            # (this file) English technical overview
├─ README.md
├─ CHANGELOG.md
├─ LICENSE
├─ requirements.txt
├─ requirements-dev.txt
├─ .gitignore
└─ pytest.ini



## 3. Data and target

Synthetic data are generated by src/data/generate_synthetic.py.

Each row represents one patient with the following columns:

patient_id — integer ID (1..N),

age — age in years,

sex — 0/1 encoding,

bmi — Body Mass Index,

smoker — 0/1 (smoking status),

diabetes — 0/1,

systolic_bp — systolic blood pressure,

heart_rate — heart rate (bpm),

cholesterol — synthetic cholesterol value,

high_risk — binary target, generated by a hidden formula.

Train/test splits are saved as:

data/processed/train.csv

data/processed/test.csv

The config file configs/default.yaml declares:

paths to train/test CSV,

ID column name,

target column name (high_risk),

list of feature columns,

model type and parameters.



## 4. Pipeline: data → features → model → metrics → API

4.1. Preprocessing (src/features/preprocess.py)

The current version assumes all features are numeric and uses:

a simple ColumnSelector to pick the desired columns;

StandardScaler to normalise them.

This is wrapped into a sklearn.Pipeline and used as the first step
in the full model pipeline.

4.2. Training (src/models/train.py)

The training script:

Loads the YAML config (configs/default.yaml).

Loads train/test data via load_train_test_from_config.

Builds a pipeline:

preprocess — feature scaling pipeline,

model — LogisticRegression with configurable hyperparameters.

Fits the pipeline on the training set.

Evaluates on the test set, computing:

ROC AUC;

average precision (PR AUC);

accuracy;

F1 score.

Saves artifacts:

trained pipeline → models/model_logistic_regression.joblib;

metrics → metrics/metrics_logistic_regression.json.


4.3. Standalone evaluation (src/models/evaluate.py)

A separate script that:

loads an already trained model from models/,

evaluates it on the test set defined in the config,

writes a compact metrics/metrics.json file for quick inspection.

This is useful when models are trained elsewhere but need to be
evaluated in a standardised way.


4.4. API layer (api/app.py)

The FastAPI application exposes:

GET /health — simple health check;

POST /predict — JSON in, risk score out.

The input schema (PatientFeatures) includes:

age, sex, bmi, smoker, diabetes,

systolic_bp, heart_rate, cholesterol.

The output schema (PredictionResponse) returns:

risk_score in 0,1,

risk_label ("low" or "high" with a default threshold 0.5),

the decision threshold,

the model file name.

Internally, the service loads the trained sklearn Pipeline from
models/model_logistic_regression.joblib and calls predict_proba.


## 5. Running the project

Basic workflow:


# 1) install dependencies
pip install -r requirements.txt

# 2) generate synthetic data
python -m src.cli generate-data --n-samples 2000

# 3) train model and compute metrics
python -m src.cli train --config default.yaml

# 4) (optional) re-evaluate saved model
python -m src.cli evaluate --config default.yaml --model-path model_logistic_regression.joblib

# 5) run tests
pytest -vv

# 6) start FastAPI server
uvicorn api.app:app --reload

-------

## 6. Intended use and limitations

The dataset is fully synthetic and based on a handcrafted “risk formula”.

The model has no clinical validation.

The project is meant to demonstrate:

a clean code structure for a tabular ML pipeline;

config-driven experiments;

basic testing and metrics logging;

a simple serving layer with FastAPI.

It must not be used for any real-world medical decision making.

